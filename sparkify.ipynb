{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Data Science Nanodegree Capstone Project\n",
    "\n",
    "* The goal of this data science project is to predict user churn, i.e., identify what causes users to create a subscription or cancel it respectively.\n",
    "* Instead of using the pandas library, we instead use the Apache Spark framework, which is build for big data analysis running on clusters.\n",
    "* The dataset used is only a fraction of the original dataset, which is about 12GB large and stored on AWS S3 (in a first step we use the small dataset; later we'll run this notebook on an AWS EMR Cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set up Spark and other Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import udf, split, trim, expr, datediff, from_unixtime\n",
    "\n",
    "from pyspark.ml.feature import RegexTokenizer, CountVectorizer, IDF, StringIndexer, Normalizer, StandardScaler, VectorAssembler\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, GBTClassifier, LinearSVC\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"mini_sparkify_event_data.json\"\n",
    "user_log = spark.read.json(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to increase width of notebook. Will be needed for SQL data operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:90% !important; }</style>\"))\n",
    "# display(HTML(\"<style>.output_result { max-width:100% !important; }</style>\"))\n",
    "display(HTML(\"<style>pre { white-space: pre !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Exploration\n",
    "\n",
    "To learn more about the users the follwoing questions need to answered:\n",
    "\n",
    "\n",
    "## Who are the Users?\n",
    "\n",
    "* What gender do users have?\n",
    "* Where do they live?\n",
    "* What are their favorite artists/songs?\n",
    "\n",
    "## How do they interact with the Platform?\n",
    "\n",
    "### Platform Usage\n",
    "\n",
    "* How often do they use the platform?\n",
    "* Do users who cancel their subscription don't use the platform or do they and might not like it?\n",
    "* How long is an average session per user?\n",
    "* What devices do they use to access the platform?\n",
    "\n",
    "### Subscription Behavior\n",
    "\n",
    "* How often do users switch their subscription level (paid/free)\n",
    "* Is there a leading indicator that a user might cancel his paid subscriptions, e.g., high count of thumbs down\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The timestamp format is not in a readable format. By creating the user defined function `get_datetime` and applying this function onto the ts column we obtain the dateTime column which has a convenient format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_datetime = udf(lambda x: datetime.datetime.fromtimestamp(x / 1000.0).strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "\n",
    "# user_log = user_log.withColumn(\"ts_iso\", get_datetime(user_log.ts)) \\\n",
    "#             .withColumn(\"registration_iso\", get_datetime(user_log.registration))\n",
    "\n",
    "# use pysparks built-in function to handel unix timestamps (divide by 1000 to convert from milliseconds)\n",
    "user_log = user_log.withColumn(\"ts_iso\", from_unixtime(user_log.ts/1000)) \\\n",
    "                .withColumn(\"registration_iso\", from_unixtime(user_log.registration/1000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The location column contains both city and state, we want to isolate both of them into seperate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('city', trim(split(user_log['location'], ',').getItem(0))) \\\n",
    "            .withColumn('state', trim(split(user_log['location'], ',').getItem(1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The userAgent columns contains information about which end-device was used to access the platform. Therfore it is usefull to learn something about the platform's users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log = user_log.withColumn('userDevice', expr(\"CASE WHEN lower(userAgent) LIKE '%windows%' THEN 'pc'\"+\n",
    "                                       \"WHEN lower(userAgent) LIKE '%macintosh%' THEN 'mac'\" +\n",
    "                                        \"WHEN lower(userAgent) LIKE '%linux%' THEN 'pc'\" +\n",
    "                                        \"WHEN lower(userAgent) LIKE '%iphone%' THEN 'mobile'\" +\n",
    "                                        \"WHEN lower(userAgent) LIKE '%ipad%' THEN 'mobile'\" +\n",
    "                                        \"ELSE 'other'\" +\n",
    "                                        \"END\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are interested in predicting user behavior, actions recorded in the dataset need to be attributed to a userid. For records without a userId we are not able to draw conclusions on the behaviour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('{} records in the dataset have no userId. They will be dropped'.format(user_log.filter(user_log.userId == '').count()))\n",
    "user_log = user_log.where(user_log.userId != '')\n",
    "print('{} records in the dataset have no userId.'.format(user_log.filter(user_log.userId == '').count()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leveraging PySpark's SQL Function for Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary view against which SQL queries can be run\n",
    "user_log.createOrReplaceTempView(\"user_log_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a table that contains user subscription behavior\n",
    "\n",
    "Idea of the query below:\n",
    "\n",
    "1. `errors_tab`\n",
    "    * Assign each entry row number to maintain order of events (have consecutive entries with exact same timestamp)\n",
    "    * Some actions are not realized because an error follows in the logs. Thus if entry $t+1$ has page \"Error\", set a flag for entry $t$ (Note that we want to keep the actual errors themselves, since they might be an indicator for users terminating their subscription, if they encounters a large number of errors during their paid regime)\n",
    "\n",
    "2. `records_tab`\n",
    "    * When level at $t$ is different from $t-1$ a new regime starts. Set a flag\n",
    "    * When level at $t$ is \"paid\" and level at $t+1$ is \"free\", the regime at $t$ is cancelled. Set a flag\n",
    "    * Choose only actions that don't result in errors\n",
    "\n",
    "3. `regime_tab`\n",
    "    * Sum up the flag set in the `records_tab`. Whenever the level changes, the `regime_count` increases by 1\n",
    "\n",
    "4. `regime_dates`\n",
    "    * The `regime_start` in the first regime is per assumption the registration date, as we don't have other data available. In later regimes $n>1$ we'll take the timestamp of the first record of regime $n$\n",
    "    * In the last regime, $N$, the `regime_end` is the max. timestamp in the dataset. Else it's the timestamp of the latest record in the particular regime $n<N$\n",
    "\n",
    "5. Final Result\n",
    "    * Collect the pre-processed data\n",
    "    * Obtain aggregate values per regime\n",
    "\n",
    "**Note:** might want to break the pretabs down into seperate SQL query commands and tables to reduce complexity of statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(\"\"\"\n",
    "            SELECT * FROM user_log_table\n",
    "            WHERE userId = 100001\n",
    "            ORDER BY userId, ts\n",
    "        \"\"\").show(5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_regime = spark.sql(\"\"\"\n",
    "                        with errors_tab AS(\n",
    "                        SELECT\n",
    "                            userId, auth, page, level, ts/1000 ts, registration/1000 registration, userDevice,\n",
    "                            row_number() OVER(PARTITION BY userId ORDER BY userId, ts) record_number,\n",
    "                            CASE WHEN LEAD(page) OVER(PARTITION BY userId ORDER BY userId, ts) = 'Error' THEN 1 ELSE 0 END leads_to_error\n",
    "                        FROM user_log_table\n",
    "                        ),\n",
    "\n",
    "\n",
    "                        records_tab as(\n",
    "                        SELECT\n",
    "                            userId, page, level, ts, registration, userDevice, record_number,\n",
    "                            CASE WHEN auth = 'Cancelled' THEN ts END date_fully_cancelled,\n",
    "                            CASE WHEN LAG(level) OVER(PARTITION BY userId ORDER BY record_number) != level THEN 1 ELSE 0 END regime_change,\n",
    "                            CASE WHEN LEAD(level) OVER(PARTITION BY userId ORDER BY record_number) = 'free' and level = 'paid' THEN 1 ELSE 0 END downgrade_regime\n",
    "                        FROM errors_tab\n",
    "                        WHERE leads_to_error = 0\n",
    "                        ),\n",
    "\n",
    "\n",
    "                        regime_tab AS(\n",
    "                        SELECT userId, page, level, ts, registration, userDevice, record_number, date_fully_cancelled, downgrade_regime,\n",
    "                            SUM(regime_change) OVER(PARTITION BY userId ORDER BY record_number)+1 regime_count\n",
    "                        FROM records_tab),\n",
    "\n",
    "\n",
    "                        regime_dates AS(            \n",
    "                        SELECT userId, page, level, userDevice, record_number, FROM_UNIXTIME(date_fully_cancelled) date_fully_cancelled, downgrade_regime, regime_count,\n",
    "                            CASE\n",
    "                                WHEN regime_count = 1 THEN FROM_UNIXTIME(registration)\n",
    "                                WHEN regime_count > 1 THEN FROM_UNIXTIME(first_value(ts) OVER(PARTITION BY userId, regime_count))\n",
    "                            END regime_start,\n",
    "                            \n",
    "                            CASE\n",
    "                                WHEN regime_count = last_value(regime_count) OVER(PARTITION BY userId ORDER BY ts) AND date_fully_cancelled IS NULL THEN FROM_UNIXTIME((select max(ts) FROM records_tab))\n",
    "                                WHEN regime_count = last_value(regime_count) OVER(PARTITION BY userId ORDER BY ts) AND date_fully_cancelled IS NOT NULL THEN FROM_UNIXTIME(date_fully_cancelled)\n",
    "                                ELSE FROM_UNIXTIME(last_value(ts) OVER(PARTITION BY userId, regime_count))\n",
    "                            END regime_end\n",
    "                        FROM regime_tab)\n",
    "\n",
    "                        SELECT\n",
    "                            DISTINCT userId,\n",
    "                            level,\n",
    "                            regime_count,\n",
    "                            DATE(regime_start) regime_start,\n",
    "                            DATE(regime_end) regime_end,\n",
    "                            DATEDIFF(day, regime_start, regime_end) regime_lenght,\n",
    "                            SUM(downgrade_regime) OVER(PARTITION BY userId, regime_count) = 1 regime_downgraded,\n",
    "                            DATE(date_fully_cancelled) date_fully_cancelled,\n",
    "                            count_if(page = 'NextSong') OVER(PARTITION BY userId, regime_count) next_song,\n",
    "                            count_if(page = 'Thumbs Up') OVER(PARTITION BY userId, regime_count) thumbs_up,\n",
    "                            count_if(page = 'Thumbs Down') OVER(PARTITION BY userId, regime_count) thumbs_down,\n",
    "                            count_if(page = 'Add Friend') OVER(PARTITION BY userId, regime_count) add_friend,\n",
    "                            count_if(page = 'Add to Playlist') OVER(PARTITION BY userId, regime_count) add_to_playlist,\n",
    "                            count_if(page = 'Save Settings') OVER(PARTITION BY userId, regime_count) save_settings,\n",
    "                            count_if(page = 'Error') OVER(PARTITION BY userId, regime_count) error,\n",
    "                            count_if(page = 'Help') OVER(PARTITION BY userId, regime_count) help,\n",
    "                            count_if(page = 'Roll Advert') OVER(PARTITION BY userId, regime_count) roll_advert,\n",
    "                            userDevice\n",
    "                        FROM regime_dates\n",
    "                        ORDER BY userId, regime_count\n",
    "                        \n",
    "                        \"\"\")\n",
    "# store results in temporary view: user_regime_table\n",
    "user_regime.createOrReplaceTempView(\"user_regime_table\")\n",
    "user_regime.show(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregate the `user_regime_table` data per User\n",
    "\n",
    "The aggregate table should have one row per user which contains all necessary information and can be used for ML model input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features= spark.sql(\"\"\"\n",
    "                        SELECT\n",
    "                            CAST(userId AS int),\n",
    "                            count_if(level = 'paid') paid_regimes,\n",
    "                            count_if(level = 'free') free_regimes,\n",
    "                            sum(regime_lenght) membership_days,\n",
    "                            count_if(regime_downgraded = 'true') downgraded_regimes,\n",
    "                            CASE WHEN max(date_fully_cancelled) IS NOT NULL THEN 1 ELSE 0 END fully_cancelled,\n",
    "                            sum(next_song) next_song,\n",
    "                            sum(thumbs_up) thumbs_up,\n",
    "                            sum(thumbs_down) thumbs_down,\n",
    "                            sum(add_friend) add_friend,\n",
    "                            sum(add_to_playlist) add_to_playlist,\n",
    "                            sum(save_settings) save_settings,\n",
    "                            sum(error) error,\n",
    "                            sum(help) help,\n",
    "                            sum(roll_advert) roll_advert,\n",
    "                            CASE WHEN count_if(userDevice = 'mac')>0 THEN 1 ELSE 0 END device_mac,\n",
    "                            CASE WHEN count_if(userDevice = 'pc')>0 THEN 1 ELSE 0 END device_pc,\n",
    "                            CASE WHEN count_if(userDevice = 'mobile')>0 THEN 1 ELSE 0 END device_mobile\n",
    "                        FROM\n",
    "                            user_regime_table\n",
    "                        GROUP BY\n",
    "                            userId\n",
    "                        ORDER BY\n",
    "                            userId\n",
    "                            \n",
    "                        \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"auth\").groupby(\"auth\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"gender\").groupby(\"gender\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"level\").groupby(\"level\").count().orderBy(\"count\", ascending=False).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"page\").groupby(\"page\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_log.select(\"userDevice\").groupby(\"userDevice\").count().orderBy(\"count\", ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "        SELECT artist, count(*) as count\n",
    "        FROM user_log_table\n",
    "        WHERE artist is not null\n",
    "        GROUP BY artist\n",
    "        ORDER BY count(*) DESC\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "        SELECT song, count(*) as count\n",
    "        FROM user_log_table\n",
    "        WHERE song is not null\n",
    "        GROUP BY song\n",
    "        ORDER BY count(*) DESC\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "        SELECT artist, song, count(*) as count\n",
    "        FROM user_log_table\n",
    "        WHERE\n",
    "            song is not null\n",
    "            AND artist is not null\n",
    "        GROUP BY artist, song\n",
    "        ORDER BY artist, count(*) DESC\n",
    "        ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_membership_days = spark.sql('''\n",
    "                                SELECT userId, max(membership_days)\n",
    "                                FROM user_log_table\n",
    "                                GROUP BY userId\n",
    "                                ORDER BY max(membership_days) DESC\n",
    "                                ''').toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_membership_days.plot(kind='hist', title='Membership in Days', legend=False, figsize=(10,6));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            SELECT userId, max(membership_days)\n",
    "            FROM user_log_table\n",
    "            WHERE page = 'Cancellation Confirmation'\n",
    "            GROUP BY userId\n",
    "            ORDER BY max(membership_days) DESC\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            with pretab as(\n",
    "            SELECT userId, max(membership_days)\n",
    "            FROM user_log_table\n",
    "            WHERE page = 'Cancellation Confirmation'\n",
    "            GROUP BY userId\n",
    "            ORDER BY max(membership_days) DESC\n",
    "            )\n",
    "            SELECT pretab.userId, count(*) records, max(membership_days)\n",
    "            FROM\n",
    "                user_log_table\n",
    "                INNER JOIN pretab on pretab.userId = user_log_table.userId\n",
    "            GROUP BY pretab.userId\n",
    "            ORDER BY max(membership_days) desc\n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancelled_userId = user_log.select('userId').filter(user_log.page == 'Cancellation Confirmation').drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cancelled_userId.createOrReplaceTempView(\"cancelled_users_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            with is_home as(\n",
    "            SELECT\n",
    "                userID, page, ts,\n",
    "                CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home                \n",
    "            FROM user_log_table\n",
    "            WHERE\n",
    "                (page = 'NextSong') or (page = 'Home')\n",
    "            ),\n",
    "            cum_sum as (\n",
    "            SELECT *, sum(is_home) OVER(PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as period\n",
    "            FROM is_home\n",
    "            )\n",
    "            SELECT AVG(count_results)\n",
    "            FROM (\n",
    "                SELECT COUNT(*) AS count_results FROM cum_sum\n",
    "                GROUP BY userID, period, page HAVING page = 'NextSong'\n",
    "            ) as counts\n",
    "                \n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            with is_home as(\n",
    "            SELECT\n",
    "                userId, auth, page, ts,\n",
    "                CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home                \n",
    "            FROM user_log_table\n",
    "            WHERE\n",
    "                (page = 'NextSong') or (page = 'Home')\n",
    "            ),\n",
    "            cum_sum as (\n",
    "            SELECT *, sum(is_home) OVER(PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as period\n",
    "            FROM is_home\n",
    "            )\n",
    "            SELECT \"cancelled_users\" as user_state, AVG(count_results) as \n",
    "            FROM (\n",
    "                SELECT COUNT(*) AS count_results FROM cum_sum WHERE userId in (select userId from cancelled_users_table)\n",
    "                GROUP BY userID, period, page HAVING page = 'NextSong'\n",
    "            ) as counts\n",
    "            \n",
    "            UNION\n",
    "            \n",
    "            SELECT \"active_users\" as user_state, AVG(count_results)\n",
    "            FROM (\n",
    "                SELECT COUNT(*) AS count_results FROM cum_sum WHERE userId not in (select userId from cancelled_users_table)\n",
    "                GROUP BY userID, period, page HAVING page = 'NextSong'\n",
    "            ) as counts\n",
    "            \n",
    "            \n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            with is_home as(\n",
    "            SELECT\n",
    "                userId, auth, page, ts,\n",
    "                CASE WHEN page = 'Home' THEN 1 ELSE 0 END AS is_home                \n",
    "            FROM user_log_table\n",
    "            WHERE\n",
    "                (page = 'NextSong') or (page = 'Home')\n",
    "            ),\n",
    "            cum_sum as (\n",
    "            SELECT *, sum(is_home) OVER(PARTITION BY userID ORDER BY ts DESC ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as period\n",
    "            FROM is_home\n",
    "            )\n",
    "            \n",
    "            select userId,  from cum_sum\n",
    "            \n",
    "                \n",
    "            ''').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql('''\n",
    "            SELECT userId, auth, ts, ts_iso, page, sessionId\n",
    "            FROM user_log_table\n",
    "            WHERE (page = 'Home' or page = 'NextSong')\n",
    "                and userId=10\n",
    "            ORDER BY userId, ts asc\n",
    "            ''').show(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todo\n",
    "\n",
    "To understand how users interact with the platform, need to know:\n",
    "\n",
    "* Songs per session\n",
    "* Session count\n",
    "* Session frequency\n",
    "    * Sessions over membership days\n",
    "    * Avg time between sessions\n",
    "\n",
    "Need to differentiate between cancelled and active users (makes sense to add `cancelled` indicator to user_log dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "* User inactivity: if a user only listens to a handful of songs per month or no songs at all, a cancellation might be likely\n",
    "    * The ratio of songs listend over time being a paying client might be a good predictor\n",
    "* Number of thumbs up/down they gave before cancelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create New Features from Aggregate User Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform Fatures for Pipeline Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the user_feature data into training and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rest, validation = user_features.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine all features into a vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputCols = user_features.columns\n",
    "inputCols.remove('fully_cancelled')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= inputCols, outputCol='inputFeatures')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Normalizer(inputCol=\"inputFeatures\", outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "In version 3.3.2 spark supports the following classification algorithms:\n",
    "\n",
    "* Logistic regression\n",
    "    * Binomial logistic regression\n",
    "    * Multinomial logistic regression\n",
    "* Decision tree classifier\n",
    "* Random forest classifier\n",
    "* Gradient-boosted tree classifier\n",
    "* Multilayer perceptron classifier\n",
    "* Linear Support Vector Machine\n",
    "* One-vs-Rest classifier (a.k.a. One-vs-All)\n",
    "* Naive Bayes\n",
    "* Factorization machines classifier\n",
    "\n",
    "We'll choose the four most popular:\n",
    "\n",
    "1. [Logistic regression](#lr)\n",
    "2. [Random forest classifier](#rf)\n",
    "3. [Gradient boosted tree classifier](#gbt)\n",
    "4. [Linear Support Vector Machine](#lsv)\n",
    "\n",
    "To **evaluate the models**, we compute the **accuracy** and the **F1-score** for each model.\n",
    "\n",
    "* Accuracy is defined as $$\\frac{\\text{# correct predictions}}{\\text{# total predictions}}$$\n",
    "* The F1-score combines the measure of accuracy and recall $$\\frac{1}{N} \\sum_{i=0}^{N-1}2\\frac{|P_i\\cap L_i|}{|P_i|\\cdot|L_i|},$$\n",
    "where $L_0, L_1, ..., L_{N-1}$ are the label sets and $P_0, P_1, ..., P_{N-1}$ the prediction sets\n",
    "\n",
    "\n",
    "Details: [Spark Documentation](https://spark.apache.org/docs/2.2.0/mllib-evaluation-metrics.html#multilabel-classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistist Regression <a id='lr'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and pipeline\n",
    "lr           =  LogisticRegression(maxIter=10, regParam=0.0, elasticNetParam=0, labelCol='fully_cancelled')\n",
    "pipeline_lr  = Pipeline(stages=[assembler, scaler, lr ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up parameter grid\n",
    "paramgrid =ParamGridBuilder()\\\n",
    ".addGrid(lr.regParam, [0.0, 0,1])\\\n",
    ".addGrid(lr.maxIter, [10])\\\n",
    ".build()\n",
    "\n",
    "# Choose f1-score as evaluation metric\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='fully_cancelled')\n",
    "\n",
    "# Set up cross validator\n",
    "crossval = CrossValidator(estimator=pipeline_lr,\n",
    "                          estimatorParamMaps=paramgrid,\n",
    "                          evaluator = evaluator , \n",
    "                          numFolds=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Fit the model\n",
    "cvModel_ls = crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply model to validation set\n",
    "results_ls = cvModel_ls.transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls_TP = (results_ls.filter(results_ls.fully_cancelled == results_ls.prediction).count())\n",
    "ls_totalNoPredictions = (results_ls.count())\n",
    "\n",
    "print('Number of Correct Predictions: {}'.format(ls_TP))\n",
    "print('Total Number of Predictions {}'.format(ls_totalNoPredictions))\n",
    "print('Accuracy {:.4f}'.format(ls_TP/ls_totalNoPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1-score: {:.4f}'.format(evaluator.evaluate(cvModel_ls.transform(validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Classifier <a id='rf'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and pipeline\n",
    "rf           = RandomForestClassifier(labelCol='fully_cancelled')\n",
    "pipeline_rf  = Pipeline(stages=[assembler, scaler, rf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid_rf = ParamGridBuilder()\\\n",
    ".build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='fully_cancelled')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_rf,  \n",
    "                          estimatorParamMaps=paramgrid_rf,\n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_rf = crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_rf = cvModel_rf.transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Random Forest Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_TP = (results_rf.filter(results_rf.fully_cancelled == results_rf.prediction).count())\n",
    "rf_totalNoPredictions = (results_rf.count())\n",
    "\n",
    "print('Number of Correct Predictions: {}'.format(rf_TP))\n",
    "print('Total Number of Predictions: {}'.format(rf_totalNoPredictions))\n",
    "print('Accuracy {:.4f}'.format(rf_TP/rf_totalNoPredictions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1-score: {:.4f}'.format(evaluator.evaluate(cvModel_rf.transform(validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Boosted Tree Classifier <a id='gbt'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and pipeline\n",
    "gbt          = GBTClassifier(labelCol='fully_cancelled')\n",
    "pipeline_gbt = Pipeline(stages=[assembler, scaler, gbt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid_gbt =ParamGridBuilder()\\\n",
    ".build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='fully_cancelled')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_gbt,\n",
    "                          estimatorParamMaps=paramgrid_gbt,\n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_gbt = crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_gbt = cvModel_gbt.transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Gradient Boosted Tree Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt_TP = (results_gbt.filter(results_gbt.fully_cancelled == results_gbt.prediction).count())\n",
    "gbt_totalNoPredictions = (results_gbt.count())\n",
    "\n",
    "\n",
    "print('Number of Correct Predictions: {}'.format(gbt_TP))\n",
    "print('Total Number of Predictions {}'.format(gbt_totalNoPredictions))\n",
    "print('Accuracy {:.4f}'.format(gbt_TP/gbt_totalNoPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1-score: {:.4f}'.format(evaluator.evaluate(cvModel_gbt.transform(validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Support Vector Machine <a id='lsv'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm = LinearSVC(labelCol='fully_cancelled')\n",
    "pipeline_svm = Pipeline(stages=[assembler, scaler, svm])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid_svm =ParamGridBuilder()\\\n",
    ".build()\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(metricName=\"f1\", labelCol='fully_cancelled')\n",
    "\n",
    "crossval = CrossValidator(estimator=pipeline_svm,  \n",
    "                          estimatorParamMaps=paramgrid_svm,\n",
    "                          evaluator=evaluator, \n",
    "                          numFolds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel_svm=crossval.fit(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_svm = cvModel_svm.transform(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy of Linear Support Vector Machine Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smv_TP = (results_svm.filter(results_svm.fully_cancelled == results_svm.prediction).count())\n",
    "smv_totalNoPredictions = (results_svm.count())\n",
    "\n",
    "print('Number of Correct Predictions: {}'.format(smv_TP))\n",
    "print('Total Number of Predictions {}'.format(smv_totalNoPredictions))\n",
    "print('Accuracy {:.4f}'.format(smv_TP/smv_totalNoPredictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('F1-score: {:.4f}'.format(evaluator.evaluate(cvModel_svm.transform(validation))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
